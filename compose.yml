services:
  # Your new FastAPI backend
  llama-swap-manager:
    build: .
    container_name: llama-swap-manager
    ports:
      - "${MANAGER_PORT:-8000}:8000"
    volumes:
      - "${MODELS_PATH:-./models}:/app/models"
      - "${CONFIG_PATH:-./config.yaml}:/app/config.yaml"
      - "./data:/app/data"
      - "./static:/app/static"
    environment:
      - LLAMA_SWAP_URL=${LLAMA_SWAP_URL:-http://llama-swap:8080}
      - MODELS_PATH=/app/models
      - CONFIG_PATH=/app/config.yaml
      - DATA_DIR=/app/data
      - MAX_FILE_SIZE=${MAX_FILE_SIZE:-50000000000}
    depends_on:
      - llama-swap
    restart: unless-stopped
    networks:
      - llama-net

  # Original llama-swap service
  llama-swap:
    image: ghcr.io/mostlygeek/llama-swap:cuda
    container_name: llama-swap2
    env_file: .env
    ports:
      - "${LLAMA_SWAP_PORT:-8090}:8080"
    volumes:
      - "${MODELS_PATH:-./models}:/models"
      - "${CONFIG_PATH:-./config.yaml}:/app/config.yaml:ro"
    restart: unless-stopped
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-all}
      - NVIDIA_DRIVER_CAPABILITIES=${NVIDIA_DRIVER_CAPABILITIES:-compute,utility}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/v1/models"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: ${GPU_COUNT:-1}
              capabilities: [gpu]
    logging:
      driver: "json-file"
      options:
        max-size: "${LOG_MAX_SIZE:-10m}"
        max-file: "${LOG_MAX_FILES:-3}"
    networks:
      - llama-net

networks:
  llama-net:
    driver: bridge